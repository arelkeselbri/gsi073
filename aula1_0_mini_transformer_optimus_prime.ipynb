{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arelkeselbri/gsi073/blob/main/mini_optimus_prime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GSI073 - Tópicos Especiais de Inteligência Artificial (Large Language Models) - Prof. Marcelo Keese Albertini\n",
        "\n",
        "Este código foi escrito em aula para demonstrar rapidamente como é a arquitetura de um Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMsbTXpunaDL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzWoCCgUndP7"
      },
      "outputs": [],
      "source": [
        "class MeuBloco(nn.Module):\n",
        "  def __init__(self, n_heads, model_dim, vocab_size):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(model_dim, bias = False)\n",
        "    self.norm2 = nn.LayerNorm(model_dim, bias = False)\n",
        "\n",
        "    self.attention = nn.MultiheadAttention(embed_dim = model_dim,\n",
        "                                           num_heads = n_heads\n",
        "                                           )\n",
        "    self.ffn = nn.Sequential(nn.Linear(model_dim, 2*model_dim),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(2*model_dim, model_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    res_atencao, _ = self.attention(x, x, x)\n",
        "    x = self.norm1(x + res_atencao)\n",
        "\n",
        "    res_ffn = self.ffn(x)\n",
        "    x = self.norm2(res_ffn + x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class MeuEncoder(nn.Module):\n",
        "  def __init__(self, n_layers, n_heads, model_dim, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, model_dim)\n",
        "    self.layers = nn.ModuleList( [\n",
        "        MeuBloco(n_heads, model_dim, vocab_size)\n",
        "        for _ in range(n_heads) ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68gnXregpJ-u"
      },
      "outputs": [],
      "source": [
        "mini_llm = MeuEncoder(n_layers = 2, n_heads = 2, model_dim = 8, vocab_size= 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCGoLPXAtUaJ"
      },
      "outputs": [],
      "source": [
        "mini_llm(torch.tensor([0,1,3,4]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh3RXNUmwMGK"
      },
      "outputs": [],
      "source": [
        "mini_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_oJbhlWwXJe"
      },
      "outputs": [],
      "source": [
        "mini_llm.embedding.weight.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0cyYs_oxIHM"
      },
      "outputs": [],
      "source": [
        "mini_llm.embedding.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcVTmdhMxuTi"
      },
      "outputs": [],
      "source": [
        "class MeuBlocoDecoder(nn.Module):\n",
        "  def __init__(self, model_dim, vocab_size, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(model_dim)\n",
        "    self.norm2 = nn.LayerNorm(model_dim)\n",
        "    self.norm3 = nn.LayerNorm(model_dim)\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, model_dim)\n",
        "\n",
        "    self.att_cros = nn.MultiheadAttention(model_dim, n_heads)\n",
        "    self.att_self = nn.MultiheadAttention(model_dim, n_heads)\n",
        "\n",
        "    self.ffn = nn.Sequential(nn.Linear(model_dim, 2*model_dim), nn.ReLU(), nn.Linear(2*model_dim, model_dim))\n",
        "\n",
        "    self.lm_head = nn.Linear(model_dim, vocab_size)\n",
        "\n",
        "    self.softmax = nn.Softmax(0)\n",
        "\n",
        "  def forward(self, x, encoder_output):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    res_att_self = self.att_self(x, x, x)\n",
        "    x = self.norm1(x + res_att_self)\n",
        "\n",
        "    res_att_cros = self.att_cros(encoder_output, encoder_output, x)\n",
        "    x = self.norm2( x + res_att_cros)\n",
        "\n",
        "    res_ffn = self.ffn(x)\n",
        "\n",
        "    x = self.norm3(x + res_ffn)\n",
        "\n",
        "    logits = self.softmax(self.lm_head(x))\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "    logits = nn.Softmax(out)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMiFg/0eIo+ENKHkHJNT3Jn",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
